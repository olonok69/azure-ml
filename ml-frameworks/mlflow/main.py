#Use MLflow with Azure Machine Learning to Train and Deploy Keras Image Classifier


# Check core SDK version number
import azureml.core
from azureml.telemetry import set_diagnostics_collection
from azureml.core.workspace import Workspace
from azureml.core.compute import ComputeTarget, AmlCompute
from azureml.core.compute_target import ComputeTargetException
import shutil
from azureml.core import Experiment, Environment, ScriptRunConfig
import sys, os
import mlflow
import mlflow.azureml

import azureml.core
from azureml.core import Workspace
import json
from mlflow.deployments import get_deploy_client

print("SDK version:", azureml.core.VERSION)
print("MLflow version:", mlflow.version.VERSION)

ws = Workspace.from_config()
ws.get_details()

print('Workspace name: ' + ws.name, 
      'Azure region: ' + ws.location, 
      'Subscription id: ' + ws.subscription_id, 
      'Resource group: ' + ws.resource_group, sep = '\n')

mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())

# choose a name for your cluster
cluster_name = "hd-cluster"

try:
    compute_target = ComputeTarget(workspace=ws, name=cluster_name)
    print('Found existing compute target')
except ComputeTargetException:
    print('Creating a new compute target...')
    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2', 
                                                           max_nodes=4)

    # create the cluster
    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)

# can poll for a minimum number of nodes and for a specific timeout. 
# if no min node count is provided it uses the scale settings for the cluster
compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)

# use get_status() to get a detailed status for the current cluster. 
print(compute_target.get_status().serialize())


experiment_name = "keras-with-mlflow"
mlflow.set_experiment(experiment_name)
lib_path = os.path.abspath("scripts")
sys.path.append(lib_path)

import train

run = train.driver()

env = Environment.get(workspace=ws, name="AzureML-TensorFlow-2.1-GPU").clone("mlflow-env")

env.python.conda_dependencies.add_pip_package("azureml-mlflow")
env.python.conda_dependencies.add_pip_package("keras==2.3.1")
env.python.conda_dependencies.add_pip_package("numpy")

src = ScriptRunConfig(source_directory="./scripts", script="train.py")
src.run_config.environment = env
src.run_config.target = cluster_name

exp = Experiment(ws, experiment_name)
run = exp.submit(src)

run.wait_for_completion(show_output=True)

# Data to be written
deploy_config ={
    "computeType": "aci"
}
# Serializing json 
json_object = json.dumps(deploy_config)
  
# Writing to sample.json
with open("deployment_config.json", "w") as outfile:
    outfile.write(json_object)

# set the tracking uri as the deployment client
client = get_deploy_client(mlflow.get_tracking_uri())

# set the model path 
model_path = "model"

# set the deployment config
deployment_config_path = "deployment_config.json"
test_config = {'deploy-config-file': deployment_config_path}

# define the model path and the name is the service name
# the model gets registered automatically and a name is autogenerated using the "name" parameter below 
client.create_deployment(model_uri='runs:/{}/{}'.format(run.id, model_path),
                         config=test_config,
                         name="keras-aci-deployment")
                         